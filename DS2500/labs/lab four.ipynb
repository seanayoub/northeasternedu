{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a0aef53",
   "metadata": {},
   "source": [
    "# Lab 4: What was that sound?\n",
    "\n",
    "### Instructions:\n",
    "- perform a fresh `restart & run all` before submitting the `.ipynb` to [gradescope](https://www.gradescope.com/courses/478298)\n",
    "- [lab rubric](https://course.ccs.neu.edu/ds2500/admin_syllabus.html?highlight=rubric#weekly-lab-ds-2501)\n",
    "- work in groups of 2-5\n",
    "- be collaborative and kind\n",
    "    - ask questions of others\n",
    "    - invite questions from others\n",
    "- each student will submit their own lab file\n",
    "- please do not share code files \n",
    "    - however, unlike HW, you're welcome to look at each other's ungraded work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a15ced",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this lab, you'll be given about 150 recordings of me saying a few different names (mostly they're pets I've had in my life, [here is Zeke, in all his glory](https://photos.app.goo.gl/6TA1mZh7dTv5cUWn6)).  Your goal is to build a classifier which can distinguish which name I'm saying.\n",
    "\n",
    "### Notes / hints:\n",
    "- avoid digging into the `lab_helper.py` file until the final part.  \n",
    "- be sure that you're comfortable with precisely what `x` and `y` are below\n",
    "    - they're arrays in sklearn format\n",
    "    - try playing a few noises\n",
    "        - see code cell just before Part A\n",
    "- [intro video](https://northeastern.zoom.us/rec/share/BmqC9mSlPwMVjR5lWFyw4_N-ws6QY9VfrVbCQILWpiKcdh8XnaqkfbP89DqYBUJo.ih5qUmin8uiyGOS5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cced036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "from lab_helper import get_df_wav, load_wav, get_xy_array\n",
    "\n",
    "folder = pathlib.Path('.') / 'sound_wav'\n",
    "df_wav = get_df_wav(folder)\n",
    "\n",
    "df_wav.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ad0d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Audio\n",
    "\n",
    "def play(x, rate):\n",
    "    \"\"\" plays a noise via jupyter\n",
    "    \n",
    "    credit: Dheeraj (chillamcharla.d@northeastern.edu), thank you!\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): 1d audio signal\n",
    "        rate (int): sampling rate\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        # mono\n",
    "        x_tup = x,\n",
    "    elif x.ndim == 2:\n",
    "        # stereo\n",
    "        x_tup = x[:, 0], x[:, 1]\n",
    "    else:\n",
    "        raise AttributeError('invalid x.ndim')\n",
    "        \n",
    "    # play audio\n",
    "    display(Audio(x_tup, rate=rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling rate of audio (error thrown if not this value)\n",
    "rate = 44100\n",
    "\n",
    "# we discard this much audio (seconds) from start of every clip\n",
    "time_trim_start = .2\n",
    "\n",
    "# clips are truncated / extended with silence to ensure they're this long (seconds)\n",
    "time_total = 1.3\n",
    "\n",
    "x, y, file_list = get_xy_array(df_wav, rate=rate, time_trim_start=time_trim_start, time_total=time_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aee3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd9d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the index of the sample below and re-run this cell to hear different noises\n",
    "play(x=x[-1, :], rate=rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44383f85",
   "metadata": {},
   "source": [
    "# Part A: Classification\n",
    "\n",
    "Build a k-NN classifier which classifies what the speaker is saying (either 'annie', 'fleck', 'linda', 'matt' or 'zeke').  Produce a complete set of cross validated estimates `y_pred`.\n",
    "\n",
    "Choose the k of k-NN arbitrarily here, we'll optimize later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c06a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "k = 10\n",
    "x_feat_list = []\n",
    "y_feat = \"word\"\n",
    "\n",
    "x = df_wav.loc[:, x_feat_list].values\n",
    "\n",
    "y_true = df_wav.loc[:, y_feat].values\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors = k)\n",
    "\n",
    "knn_classifier.fit(x, y_true)\n",
    "\n",
    "y_pred = knn_classifier.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2383569a",
   "metadata": {},
   "source": [
    "# Part B: Confusion Matrix\n",
    "\n",
    "Plot a confusion matrix which shows the performance of our initial classifier.  In a sentence or two, summarize what the confusion matrix shows is happening.  \n",
    "\n",
    "Our initial classifier, as per usual, may not work too well.  Thanks ok!  Real ML applications work like this:\n",
    "1. try to build something\n",
    "1. see if it works\n",
    "1. if it doesn't work:\n",
    "    1. find out why\n",
    "    1. fix it\n",
    "    1. repeat from step 2 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "conf_mat = confusion_matrix(y_true = y_true, y_pred = y_pred)\n",
    "conf_mat_disp = ConfusionMatrixDisplay(conf_mat, display_labels=np.unique(y_true))\n",
    "\n",
    "sns.set(font_scale = 2)\n",
    "conf_mat_disp.plot()\n",
    "plt.suptitle(\".wav classification\")\n",
    "plt.gcf().set_size_inches(7, 7)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76288c0e",
   "metadata": {},
   "source": [
    "# Part C: Tuning up our classifier and data\n",
    "\n",
    "[part c video](https://northeastern.zoom.us/rec/share/kPUjJd-2-lw1skuqw5Q-4iBHHWQFBSNtOMNvLSHvmlPBkM0-eB_PHp84SJFwtvGe.JH1ylku-jldDQGMq)\n",
    "\n",
    "The classifier in Part B didn't work too well ... did it?  Usually our ML approaches will not do their best right away, the challenge is to understand why and see what we can do to tune it up.  The plot below shows each noise of every category, with unique samples overlaid in different colors:\n",
    "\n",
    "<img src=\"https://i.ibb.co/MC2CrNp/noise-initial.png\" width=800>\n",
    "\n",
    "\n",
    "Examining the initial plots of our data, we see that there's a few problems with the input data.  Fix these problems:\n",
    "1. The volume differs per sample\n",
    "    - This is not appropriate.  Implicitly, just because one noise is louder or queiter shouldn't make it more or less likely to be a particular word.\n",
    "    - How can we make the volume the same for all samples?  \n",
    "        - Hint: see scale normalization notes\n",
    "2. Noises, even from a single class, all start at different times!\n",
    "    - Due to inconsistencies in my ability to record, the noise \"Fleck\" starts a bit earlier or later in the recording (notice that different samples of \"Fleck\" are shifted left / right from each other in the image below)\n",
    "    - This is not appropriate. Ideally the first utterance of the \"F\" noise should occur at the same index in each recording\n",
    "    - Is there some way we can \"chop-off\" the beggining of each noise, up to the point where it first begins?  \n",
    "        - Doing so would \"align\" our noises so we don't have this shifted behavior below.\n",
    "        - I found [np.where](https://numpy.org/doc/stable/reference/generated/numpy.where.html) helpful, other approachs work too\n",
    "<img src=\"https://i.ibb.co/0VynmD4/fleck-detail-no-time-align.png\" width=400>\n",
    "\n",
    "Hint for these first two issues: I'd suggest modifying the function `load_wav()` in `lab_helper.py`.  You can submit this alongside the lab to show your work.\n",
    "\n",
    "3. There's a few parameters we can tune to make sure our classifier is best:\n",
    "    - the \"k\" of k nearest neighbors\n",
    "        - hint: see day13 ica2 for an example\n",
    "    - the total length of time each sample runs for\n",
    "    \n",
    "To see if you've made progress in resolving the problems above, rebuild a cross validated confusion matrix and see if your performance improves.  (Hint: if you resolve the first two problems above the classifier performs reasonably well)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
